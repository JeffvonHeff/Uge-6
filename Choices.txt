Designvalg for ETL-processen
===========================

Dette dokument opsummerer de vigtigste implementeringsvalg i hvert trin af den
nuværende ETL-pipeline og fremhæver fornuftige alternativer, der kan erstatte den
eksisterende tilgang, hvis fremtidige krav ændrer sig.

Extract
-------
- **Nuværende valg:** `Extract.extract_data` indlæser hver CSV-fil fra disken i
en pandas-ordbog med `DataFrame`-objekter ved hjælp af et fast mapping mellem
tablenavne og filstier samt et enkelt valgfrit `base_path`-argument. Alle filer
læses eager med `pandas.read_csv`, hvor strengen ``"NULL"`` fortolkes som
manglende data, og hjælperen udskriver den fulde sti for sporbarhed.
- **Det kunne vi have gjort i stedet:**
  - Indføre konfigurationsstyret filopslag (f.eks. miljøvariabler eller et YAML-/
    JSON-manifest) for at undgå hardkodede relative stier.
  - Streame store filer i bidder eller bruge lazy-iteratorer for at reducere
    peak-hukommelsesforbruget, hvis datamængden vokser.
  - Erstatte rå CSV-indlæsning med API-/databaseudtræk, især hvis kildesystemerne
    udstiller REST-endpoints eller direkte databaseadgang.
  - Pakke extraction ind i retry-/overvågningslogik eller bruge orkestrerings-
    værktøjer (Airflow, Prefect), når vi kører i mindre kontrollerede miljøer.

Transform
---------
- **Nuværende valg:** `Transform.prepare_relational_tables` udfører al rengøring
og relationel justering med in-memory pandas-transformationer. Funktionen
omdøber kolonner, håndhæver numeriske typer, opbygger surrogatnøgler til stores
og staff og joiner opslagsværdier, før den returnerer almindelige
`DataFrame`-objekter. `build_order_summary` aggregerer derefter ordretotaler med
vektoriserede pandas-operationer og fletter kundeoplysninger ind til rapportering.
- **Det kunne vi have gjort i stedet:**
  - Bruge et dedikeret lag til skemavalidering (f.eks. pandera eller pydantic
    dataframes) for at håndhæve constraints eksplicit og give tydeligere fejl.
  - Udføre transformationerne med SQL i en staging-database eller via dbt, hvis vi
    foretrækker database-native processer og afhængighedssporing.
  - Opdele den monolitiske transform-funktion i mindre, testbare trin eller bruge
    et deklarativt pipeline-bibliotek (f.eks. Dagster, Prefect tasks) for at øge
    genbrugelighed og observabilitet.
  - Cache mellemtabeller eller udnytte Apache Arrow/Parquet-formater, når de samme
    afledte datasæt genbruges på tværs af downstream-jobs.

Load
----
- **Nuværende valg:** `Load.create_connection` henter forbindelsesindstillinger
fra miljøvariabler og åbner en psycopg2-forbindelse. `load_core_tables`
trunkerer eksisterende relationelle tabeller, bulk-indlæser rækker med
`execute_values` og nulstiller serielle sekvenser, mens `load_order_summary`
dropper og genskaber rapporttabellen før indsættelse af hver række med
`cursor.executemany` inde i en eksplicit transaktion.
- **Det kunne vi have gjort i stedet:**
  - Overlade forbindelseshåndtering til SQLAlchemy eller en anden ORM/pooler for
    at håndtere retries, pooling og forskellige databaser mere elegant.
  - Erstatte fuld truncering med inkrementelle eller upsert-baserede load-
    strategier, når datamængden stiger, eller historiske data skal bevares.
  - Skifte manuelle SQL-strenge ud med migrations-/versionsværktøjer (Alembic,
    Flyway), så schemaændringer spores og kan reproduceres.
  - Bruge databasespecifikke bulk-load-værktøjer (f.eks. PostgreSQL `COPY`,
    cloud-datalager loaders) for hurtigere indlæsning, når datasæt vokser ud over
    demo-størrelsen.
