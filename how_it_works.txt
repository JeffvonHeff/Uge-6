How the ETL project works (step by step)
=======================================

1. Configure the PostgreSQL connection
   -----------------------------------
   The loader reads database settings from ``POSTGRES_*`` environment variables.
   ``main.py`` calls ``Load.set_postgres_env`` so sensible defaults are applied
   automatically when those variables are missing, which keeps the demo flow
   simple. Override a value by exporting your own environment variable (or by
   passing ``overwrite=True`` to ``set_postgres_env`` if you integrate the
   helper elsewhere). ``Load.write_postgres_env`` can persist the active settings
   to a ``.env`` file for reuse with Docker Compose or other tooling.

2. Run the pipeline entry point
   ----------------------------
   Execute ``python main.py`` (or ``python main.py run``) from the repository
   root. The command-line parser also exposes inspection helpers:

   * ``python main.py tables`` lists tables and row counts.
   * ``python main.py preview orders --limit 10`` prints a sample of rows as a
     nicely formatted pandas table.
   * ``python main.py overview`` combines both ideas and shows a preview for
     every user table.

3. Extract the raw datasets
   ------------------------
   ``Extract.extract_all`` downloads the JSON payloads defined in
   ``Extract.API_ENDPOINTS`` using ``requests``. Each payload is converted to a
   pandas ``DataFrame`` and written to disk as ``<name>.csv``. If the API is
   unreachable, the helper falls back to any previously cached CSV files so the
   rest of the pipeline can still run offline.

4. Transform the DataFrames
   ------------------------
   ``Transform.transform_dataframes`` normalises date columns in the orders data
   and calculates a ``line_total`` per order item. The function returns two
   tables:

   * ``orders`` - cleaned order data with parsed datetimes.
   * ``order_summary`` - order totals joined with customer information.

5. Load the transformed data
   -------------------------
   ``Load.build_connection_from_env`` opens a psycopg2 connection using the
   environment variables above. If the target database does not yet exist, the
   module creates it automatically via the ``postgres`` maintenance database.
   ``Load.load_dataframes`` then:

   * drops the destination table if it exists,
   * recreates it with types inferred from the pandas dtypes,
   * coerces pandas-specific nulls (``NaN`` or ``NaT``) to real SQL ``NULL``,
   * bulk inserts every row with ``cursor.executemany``.

   All SQL runs within a transaction so either every table loads successfully or
   nothing changes.

6. Review the results
   ------------------
   After the load step completes, ``main.py`` automatically prints a database
   snapshot by reusing the inspection utilities from ``postgres_env.py``. The
   output is rendered via pandas so the column widths align for easy scanning.
   You can rerun the same overview, table listing, or previews at any time using
   the command-line options described above.
