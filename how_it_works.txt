How the tiny ETL project works
==============================

1. Read the CSV files
   -------------------
   ``Extract.extract_data`` opens the three CSV files that ship with the project
   (orders, order items, and customers). Nothing tricky happens here – pandas
   simply reads the rows into DataFrames so we can play with them in memory.

2. Build a friendly order summary
   ------------------------------
   ``Transform.build_order_summary`` makes the data easier to understand:

   * Every order date string like ``01/01/2016`` is turned into a real date
     object.
   * Each ``order_items`` row gets a ``line_total`` value (quantity × price ×
     discount).
   * Totals are added up per order and combined with the matching customer
     name.

   The function returns one tidy table with the columns we care about most:
   ``order_id``, ``order_date``, ``customer_id``, ``customer_name``, and
   ``order_total``.

3. Save the summary to PostgreSQL
   -------------------------------
   ``Load.create_connection`` opens a connection using the ``POSTGRES_*``
   environment variables (defaults are provided for quick tests). Then
   ``Load.load_order_summary`` drops the ``order_summary`` table if it already
   exists, recreates it with a super simple schema, and inserts every row from
   the summary DataFrame.

4. Run everything from ``main.py``
   -------------------------------
   ``main.run_pipeline`` calls the three steps above, prints progress messages,
   and shows the first few rows of the summary so you can see what will be saved
   in the database. Run ``python main.py`` and you are done.
