How the ETL project works (step by step)
=======================================

1. Configure the PostgreSQL connection
   ------------------------------------
   Set the following environment variables so the loader knows how to reach your
   PostgreSQL instance:

   * ``POSTGRES_USER`` – database user name
   * ``POSTGRES_PASSWORD`` – password for the user
   * ``POSTGRES_HOST`` – hostname or IP of the PostgreSQL server
   * ``POSTGRES_PORT`` – port number (use ``5432`` for the default)
   * ``POSTGRES_DATABASE`` – database name that should receive the data

2. Run the pipeline entry point
   -----------------------------
   Execute ``python main.py`` from the repository root.

   * ``Extract.extract_all`` downloads the JSON payload from the three public
     API endpoints listed in ``Extract.API_ENDPOINTS``.
   * Each payload becomes a pandas DataFrame and a CSV file stored locally.

3. Transform the raw DataFrames
   -----------------------------
   ``Transform.transform_dataframes`` normalises the order dates and calculates
   a ``line_total`` for each order item. The function returns:

   * ``orders`` – the cleaned order data
   * ``order_summary`` – order totals joined with customer information

4. Load the transformed data
   --------------------------
   ``Load.load_dataframes`` connects directly to PostgreSQL (no ORM). For every
   DataFrame it:

   * drops the existing table if it is present
   * recreates the table with simple column types that match the DataFrame
   * inserts all rows using plain ``INSERT`` statements

   All SQL runs inside a transaction so either every table loads successfully or
   nothing changes.

5. Review the results
   -------------------
   After the script completes, inspect the PostgreSQL tables ``orders`` and
   ``order_summary`` to confirm the data is present.
