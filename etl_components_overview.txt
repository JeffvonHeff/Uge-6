ETL Pipeline Component Overview
===============================

The project now contains four tiny Python modules that keep the ETL story very
simple.

Extract (``Extract.py``)
------------------------
* **Purpose:** Read the CSV files that live in the repository.
* **Key function:** ``extract_data`` loads ``orders.csv``, ``order_items.csv``,
  and ``customers.csv`` into pandas DataFrames and returns them in a dictionary.

Transform (``Transform.py``)
----------------------------
* **Purpose:** Build a single, human-friendly table from the raw pieces.
* **Key function:** ``build_order_summary`` parses the order dates, calculates an
  ``order_total`` per order, and adds a ``customer_name`` column so every row is
  easy to understand.

Load (``Load.py``)
------------------
* **Purpose:** Save the summary table to PostgreSQL using very small SQL
  statements.
* **Key function:** ``load_order_summary`` drops and recreates the
  ``order_summary`` table, then inserts each row from the summary DataFrame. The
  helper relies on ``create_connection``, which reads simple ``POSTGRES_*``
  environment variables (or uses defaults) to connect with psycopg2.

Main entry point (``main.py``)
-----------------------------
* **Purpose:** Glue the three steps together.
* **Execution flow:** ``run_pipeline`` extracts the CSV data, builds the summary
  table, shows the first few rows, and finally loads the data into PostgreSQL.
  Running ``python main.py`` performs the whole process end-to-end.
